{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run import *\n",
    "from utils import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"7sd8l0oc\"\n",
    "root_dir = os.path.join(\"..\", \"results\", \"DPT_2\", run_name)\n",
    "checkpoint_file = get_best_checkpoint(root_dir, 'epoch')\n",
    "\n",
    "model = DPTSolver.load_from_checkpoint(checkpoint_file).cpu()#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_offline_dataset: 4000\n",
      "val_offline_dataset: 1000\n",
      "val_online_dataset: 1000\n"
     ]
    }
   ],
   "source": [
    "config = model.config\n",
    "config['batch_size'] = 1\n",
    "\n",
    "dl = get_dataloaders(config)\n",
    "\n",
    "train_offline_dataloader = dl[\"train_dataloaders\"]\n",
    "val_offline_dataloader = dl[\"val_dataloaders\"][0]\n",
    "val_online_dataloader = dl[\"val_dataloaders\"][1]\n",
    "\n",
    "train_offline_dataset = train_offline_dataloader.dataset\n",
    "val_offline_dataset = val_offline_dataloader.dataset\n",
    "val_online_dataset = val_online_dataloader.dataset\n",
    "\n",
    "train_offline_batch = next(iter(train_offline_dataloader))\n",
    "val_offline_batch = next(iter(val_offline_dataloader))\n",
    "val_online_batch = next(iter(val_online_dataloader))\n",
    "\n",
    "train_offline_sample = train_offline_dataset[0]\n",
    "val_offline_sample = val_offline_dataset[0]\n",
    "val_online_sample = val_online_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "prob_dist must be 1 or 2 dim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m      2\u001b[0m     x\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \n\u001b[1;32m      3\u001b[0m     y\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/BANANASTЯ/DPT/src/train.py:67\u001b[0m, in \u001b[0;36mDPTSolver.get_predictions\u001b[0;34m(self, outputs, do_sample, temperature)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample \u001b[38;5;129;01mand\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(outputs \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# input_dim = 1, output_dim = 1\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: prob_dist must be 1 or 2 dim"
     ]
    }
   ],
   "source": [
    "outputs = model.model(\n",
    "    x=sample[\"x\"].unsqueeze(0), \n",
    "    y=sample[\"y\"].unsqueeze(0)\n",
    ")\n",
    "predictions = model.get_predictions(outputs, sample[\"x_min\"].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_offline_batch = next(iter(train_offline_dataloader))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offline_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_offline_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_loss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs) \u001b[38;5;241m|\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metrics(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m, outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Desktop/BANANASTЯ/DPT/src/train.py:84\u001b[0m, in \u001b[0;36mDPTSolver._offline_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_offline_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 84\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs,\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_predictions(outputs),\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_min\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     89\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/envs/DPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DPT/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/BANANASTЯ/DPT/src/model.py:64\u001b[0m, in \u001b[0;36mDPT.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     61\u001b[0m y_emb \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# [batch_size, seq_len + 1, input_dim]\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# [batch_size, seq_len + 1, 1]\u001b[39;00m\n\u001b[1;32m     76\u001b[0m y_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     77\u001b[0m     [\n\u001b[1;32m     78\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     86\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# train_offline_batch = next(iter(train_offline_dataloader))\n",
    "outputs = model._offline_step(train_offline_sample)\n",
    "metrics = model.get_loss(**outputs) | model.get_metrics(**outputs)\n",
    "\n",
    "print('predictions', outputs['predictions'].shape, outputs['predictions'].dtype)\n",
    "print('targets', outputs['targets'].shape, outputs['targets'].dtype)\n",
    "for key, val in metrics.items():\n",
    "    print(f'{key}: {val.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions torch.Size([1, 4, 1]) torch.float32\n",
      "targets torch.Size([1, 1]) torch.float32\n",
      "loss: 0.00210\n",
      "accuracy: 0.00000\n",
      "mae: 0.04579\n"
     ]
    }
   ],
   "source": [
    "val_offline_batch = next(iter(val_offline_dataloader))\n",
    "outputs = model._offline_step(val_offline_batch)\n",
    "metrics = model.get_loss(**outputs) | model.get_metrics(**outputs)\n",
    "\n",
    "print('predictions', outputs['predictions'].shape, outputs['predictions'].dtype)\n",
    "print('targets', outputs['targets'].shape, outputs['targets'].dtype)\n",
    "for key, val in metrics.items():\n",
    "    print(f'{key}: {val.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions torch.Size([1, 4, 1]) torch.float32\n",
      "targets torch.Size([1, 1]) torch.float32\n",
      "loss: 0.00141\n",
      "accuracy: 0.00000\n",
      "mae: 0.03761\n"
     ]
    }
   ],
   "source": [
    "val_online_batch = next(iter(val_online_dataloader))\n",
    "outputs = model._online_step(val_online_batch)\n",
    "results = model.get_loss(**outputs) | model.get_metrics(**outputs)\n",
    "\n",
    "print('predictions', outputs['predictions'].shape, outputs['predictions'].dtype)\n",
    "print('targets', outputs['targets'].shape, outputs['targets'].dtype)\n",
    "for key, val in metrics.items():\n",
    "    print(f'{key}: {val.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed: 0 suggested: x = 0.435287, y = 0.580127\n",
      "observed: 1 suggested: x = 0.020918, y = 0.495109\n",
      "observed: 2 suggested: x = 0.103798, y = 0.471585\n",
      "observed: 3 suggested: x = 0.094196, y = 0.473273\n",
      "groun truth: x = 0.127678, y = 0.932751\n",
      "loss 0.001414\n",
      "accuracy 0.000000\n",
      "mae 0.037607\n"
     ]
    }
   ],
   "source": [
    "transform = lambda x: ''.join([f'{x:.6f}' for x in x.tolist()])\n",
    "\n",
    "for i, x in enumerate(outputs[\"predictions\"][0]):\n",
    "    y = val_online_batch[\"problem\"][0].target(x.detach().numpy())\n",
    "    print(f'observed: {i} suggested: x = {transform(x)}, y = {y:.6f}')\n",
    "print(f'groun truth: x = {transform(sample[\"problem\"].info[\"x_min\"])}, y = {sample[\"problem\"].info[\"y_min\"]:.6f}')\n",
    "\n",
    "for key, val in metrics.items():\n",
    "    print(key, f'{val.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed: 0 suggested: x = 0.435287, y = 0.122911\n",
      "observed: 1 suggested: x = 0.098243, y = 1.670192\n",
      "observed: 2 suggested: x = 0.716114, y = 0.921985\n",
      "observed: 3 suggested: x = 0.731917, y = 1.023368\n",
      "groun truth: x = 0.457487, y = 0.116980\n",
      "accuracy 0.000000\n",
      "mae 0.274430\n",
      "loss 0.075312\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(0, len(val_online_dataset))\n",
    "sample = val_online_dataset[i]\n",
    "results = model.run(\n",
    "    problem=sample[\"problem\"],\n",
    "    n_steps=4\n",
    ")\n",
    "outputs_unsqueezed = results[\"outputs\"].unsqueeze(0), sample[\"x_min\"].unsqueeze(0), results[\"x\"].unsqueeze(0)\n",
    "metrics = model.get_metrics(*outputs_unsqueezed) | model.get_loss(*outputs_unsqueezed)\n",
    "\n",
    "transform = lambda x: ''.join([f'{x:.6f}' for x in x.tolist()])\n",
    "for i, (x, y) in enumerate(zip(results[\"x\"], results[\"y\"])):\n",
    "    print(f'observed: {i} suggested: x = {transform(x)}, y = {y:.6f}')\n",
    "print(f'groun truth: x = {transform(sample[\"problem\"].info[\"x_min\"])}, y = {sample[\"problem\"].info[\"y_min\"]:.6f}')\n",
    "\n",
    "for key, val in metrics.items():\n",
    "    print(key, f'{val.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"outputs\"].shape\n",
    "sample[\"x_min\"].shape\n",
    "results[\"x\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
