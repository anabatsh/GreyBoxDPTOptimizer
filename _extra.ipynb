{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpt_run import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers.dpt.utils.data import MarkovianOfflineDataset, load_markovian_learning_histories\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories_solv = load_markovian_learning_histories(\"../GreyBoxDPTOptimizerData/trajectories/Net_d_2_n_4_solvers\")\n",
    "# histories_grid = load_markovian_learning_histories(\"../GreyBoxDPTOptimizerData/trajectories/Quadratic_d_1_n_1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories = {}\n",
    "# for history in histories_solv:\n",
    "#     problem = int(re.search(r'\\d+', history['name']).group())\n",
    "#     # print(history['name'], problem)\n",
    "#     if problem not in trajectories.keys():\n",
    "#         trajectories[problem] = []\n",
    "#     trajectories[problem].append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distr = np.zeros((len(trajectories), 16))\n",
    "# for i, histories_list in trajectories.items():\n",
    "#     for histories in histories_list:\n",
    "#         indx, counts = np.unique(histories['actions'], return_counts=True)\n",
    "#         distr[i-1][indx] += counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackig(dataset, n_actions=1, n_epochs=1):\n",
    "    # distr = np.zeros((280, 16))\n",
    "    track = {i: {'uniq_rate': [], 'targ_rate': []} for i in range(280)}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for sample in dataset:\n",
    "            i = int(re.search(r'\\d+', sample['name']).group())\n",
    "            uniq_rate = len(np.unique(sample['actions'])) / n_actions\n",
    "            targ_rate = (sample['actions'] == sample['target_action']).to(torch.float32).mean()\n",
    "            track[i-1]['uniq_rate'].append(uniq_rate)\n",
    "            track[i-1]['targ_rate'].append(targ_rate)\n",
    "            # indx, counts = np.unique(sample['actions'], return_counts=True)\n",
    "            # distr[i-1][indx] += counts\n",
    "            # track[i-1].append(sum(sample['actions'] == sample['target_action']).item())\n",
    "\n",
    "    for i, rates in track.items():\n",
    "        for name, rate in rates.items():\n",
    "            track[i][name] = {\n",
    "                'min': np.min(rate).item(),\n",
    "                'max': np.max(rate).item(),\n",
    "                'mean': np.mean(rate).item(),\n",
    "                'std': np.std(rate).item()\n",
    "            }\n",
    "\n",
    "    total = {}\n",
    "    for i, rates in track.items():\n",
    "        for name, rate in rates.items():\n",
    "            for key, val in rate.items():\n",
    "                if name not in total.keys():\n",
    "                    total[name] = {}\n",
    "                if key not in total[name].keys():\n",
    "                    total[name][key] = []\n",
    "                total[name][key].append(val)\n",
    "\n",
    "    for name, values in total.items():\n",
    "        for key, val in values.items():\n",
    "            total[name][key] = np.mean(total[name][key]).item()\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dataloaders(load_config('dpt_run_config.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_net_2_4_50_solv = MarkovianOfflineDataset(\"../GreyBoxDPTOptimizerData/trajectories/Net_n_2_d_4_solvers\", seq_len=50)\n",
    "# dataset_net_2_4_50_grid = MarkovianOfflineDataset(\"../GreyBoxDPTOptimizerData/trajectories/Net_n_2_d_4_grid\", seq_len=50)\n",
    "# dataset_net_2_4_8_grid = MarkovianOfflineDataset(\"../GreyBoxDPTOptimizerData/trajectories/Net_n_2_d_4_grid\", seq_len=8)\n",
    "# dataset_net_2_8_100_grid = MarkovianOfflineDataset(\"../GreyBoxDPTOptimizerData/trajectories/Net_n_2_d_8_grid\", seq_len=100)\n",
    "# dataset_net_2_10_100_grid = MarkovianOfflineDataset(\"../GreyBoxDPTOptimizerData/trajectories/Net_n_2_d_10_grid\", seq_len=100)\n",
    "# dataset_quadratic_1024_1_100_grid = MarkovianOfflineDataset(\"../GreyBoxDPTOptimizerData/trajectories/Quadratic_n_1024_d_1_grid\", seq_len=100)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_net_2_4_50_solv = trackig(dataset_net_2_4_50_solv, n_actions=16, n_epochs=1)\n",
    "# total_net_2_4_50_grid = trackig(dataset_net_2_4_50_grid, n_actions=16, n_epochs=60)\n",
    "# total_net_2_4_8_grid = trackig(dataset_net_2_4_8_grid, n_actions=16, n_epochs=60)\n",
    "# total_net_2_8_100_grid = trackig(dataset_net_2_8_100_grid, n_actions=256, n_epochs=60)\n",
    "# total_net_2_10_100_grid = trackig(dataset_net_2_10_100_grid, n_actions=1024, n_epochs=60)\n",
    "total_quadratic_1024_1_100_grid = trackig(dataset_quadratic_1024_1_100_grid, n_actions=1024, n_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "values = [\n",
    "    ['net', '2', '4', '50', f\"{total_net_2_4_50_solv['uniq_rate']['mean'] * 100:.1f}%\", f\"{total_net_2_4_50_solv['targ_rate']['mean'] * 100:.1f}%\"], \n",
    "    ['net', '2', '4', '50', f\"{total_net_2_4_50_grid['uniq_rate']['mean'] * 100:.1f}%\", f\"{total_net_2_4_50_grid['targ_rate']['mean'] * 100:.1f}%\"],\n",
    "    ['net', '2', '4', '8', f\"{total_net_2_4_8_grid['uniq_rate']['mean'] * 100:.1f}%\", f\"{total_net_2_4_8_grid['targ_rate']['mean'] * 100:.1f}%\"],\n",
    "    ['net', '2', '8', '100', f\"{total_net_2_8_100_grid['uniq_rate']['mean'] * 100:.1f}%\", f\"{total_net_2_8_100_grid['targ_rate']['mean'] * 100:.1f}%\"],\n",
    "    ['net', '2', '10', '100', f\"{total_net_2_10_100_grid['uniq_rate']['mean'] * 100:.1f}%\", f\"{total_net_2_10_100_grid['targ_rate']['mean'] * 100:.1f}%\"],\n",
    "    # ['quadratic', '1024', '1', '100', total_quadratic_1024_1_100_grid['uniq_rate']['mean'], total_quadratic_1024_1_100_grid['targ_rate']['mean']],\n",
    "]\n",
    "pd.DataFrame(values, columns=['problem', 'n', 'd', 'seq_len', 'uniq', 'targ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_problem(problems.Quadratic(d=1, n=1024, seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MarkovianOfflineDataset(\"../GreyBoxDPTOptimizerData/trajectories/SimpleQuadratic_n_1024_d_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]['actions'], df[0]['target_action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('dpt_run_config.yaml')\n",
    "dataloaders = get_dataloaders(config)\n",
    "\n",
    "train_offline_batch = next(iter(dataloaders['train_dataloaders']))\n",
    "for key, val in train_offline_batch.items():\n",
    "    if key != 'name':\n",
    "        print(f'{key:>13} {list(val.shape)}')\n",
    "print()\n",
    "\n",
    "val_offline_batch = next(iter(dataloaders['val_dataloaders'][0]))\n",
    "for key, val in val_offline_batch.items():\n",
    "    if key != 'name':\n",
    "        print(f'{key:>13} {list(val.shape)}')\n",
    "\n",
    "# print()\n",
    "# val_online_batch = next(iter(dataloaders['val_dataloaders'][1]))\n",
    "# for key, val in val_online_batch[0].items():\n",
    "#     try:\n",
    "#         print(f'{key:>13} {list(val.shape)}')\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPTSolver(config)\n",
    "model._offline_step(train_offline_batch, 0)\n",
    "\n",
    "model.eval()\n",
    "model._offline_step(val_offline_batch, 0)\n",
    "# model._online_step(val_online_batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python dpt_run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPT Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "\n",
    "from solvers.dpt.train import DPTSolver\n",
    "from problems import Net\n",
    "from dpt_run import get_online_dataloader, load_config\n",
    "from utils import print_trajectory\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from problems import Net\n",
    "# from utils import *\n",
    "\n",
    "# problem = Net(d=4, n=2, seed=0)\n",
    "# all_actions = get_xaxis(d=problem.d, n=problem.n)\n",
    "# all_states = problem.target(all_actions)\n",
    "# print(np.array([all_states.max()]).shape)\n",
    "# print(np.empty(shape=(1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = torch.randn(size=(1, 10))\n",
    "# # plt.plot(logits[0])\n",
    "\n",
    "# for t in (0.01, 0.5, 1.0, 1.5, 2.0, 5.0, 100.0):\n",
    "#     probs_t = torch.nn.functional.softmax(logits / t, dim=-1)\n",
    "#     plt.plot(probs_t[0], label=t)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('config.yaml')\n",
    "\n",
    "d = int(np.log2(config[\"model_params\"][\"num_actions\"]))\n",
    "problems = [Net(d=d, n=2, seed=i) for i in range(271, 281)]\n",
    "val_online_dataloader = get_online_dataloader(problems, config)\n",
    "print('val_online_dataloader:', len(val_online_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = '../GreyBoxDPTOptimizerData/GreyBoxDPTOptimizer/0cxrd4bf/checkpoints/epoch=299-step=9600.ckpt'\n",
    "model = DPTSolver.load_from_checkpoint(checkpoint_file).to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_temperature(temperature_function):\n",
    "    visited_points = []\n",
    "    convergency_step = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        for batch in val_online_dataloader:\n",
    "            for sample in batch:\n",
    "                result = model.run(\n",
    "                    **sample,\n",
    "                    n_steps=config[\"model_params\"][\"seq_len\"],\n",
    "                    return_trajectory=True,\n",
    "                    temperature_function=temperature_function\n",
    "                )\n",
    "                right_action_indexes = torch.where(result[\"actions\"] == sample[\"target_action\"])[0]\n",
    "                if len(right_action_indexes):\n",
    "                    right_action_step = right_action_indexes[0].item() + 1\n",
    "                    visited_points_number = len(torch.unique(result[\"actions\"][:right_action_step]))\n",
    "                else:\n",
    "                    right_action_step = np.nan\n",
    "                    visited_points_number = np.nan\n",
    "                convergency_step.append(right_action_step)\n",
    "                visited_points.append(visited_points_number)\n",
    "            # print(model._online_step(batch, 0))\n",
    "            # break\n",
    "    mask = np.isnan(convergency_step)\n",
    "    if mask.all():\n",
    "        convergency_step = np.nan\n",
    "        visited_points = np.nan\n",
    "    else:\n",
    "        convergency_step = np.nanmean(convergency_step)\n",
    "        visited_points = np.nanmean(visited_points)\n",
    "    return convergency_step, visited_points, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_results = {}\n",
    "temperature_functions = {\n",
    "   'argmax': lambda x: 0,\n",
    "    't = 0.00001': lambda x: 0.00001,\n",
    "    't = 0.5': lambda x: 0.5,\n",
    "    't = 1.0': lambda x: 1.0, \n",
    "    't = 10':  lambda x: 10,\n",
    "    't = 100': lambda x: 100,\n",
    "    't ~ 1 - n_step / n_steps': lambda x: 1 - x / config[\"model_params\"][\"seq_len\"],\n",
    "    't ~ 10 * (1 - n_step / n_steps)': lambda x: 10 * (1 - x / config[\"model_params\"][\"seq_len\"]),\n",
    "    't ~ 100 * (1 - n_step / n_steps)': lambda x: 100 * (1 - x / config[\"model_params\"][\"seq_len\"]) \n",
    "}\n",
    "for name, temperature_function in temperature_functions.items():\n",
    "   convergency_step, visited_points, mask = check_temperature(temperature_function)\n",
    "   temperature_results[name] = dict(\n",
    "      convergency_step=convergency_step, \n",
    "      visited_points=visited_points, \n",
    "      mask=mask,\n",
    "      temperature_function=temperature_function\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = config[\"model_params\"][\"seq_len\"]\n",
    "n_step_range = np.arange(0, seq_len)\n",
    "num_actions = config[\"model_params\"][\"num_actions\"]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for name, result in temperature_results.items():\n",
    "    print(f'Temperature function: {name}')\n",
    "    if result['mask'].all():\n",
    "        print('Has not converged')\n",
    "    else:\n",
    "        s_1 = f'Converged at {result[\"convergency_step\"]:.1f}/{seq_len} step'\n",
    "        s_2 = f' in {np.sum(~result[\"mask\"])}/{len(result[\"mask\"])} cases'\n",
    "        s_3 = f' and visited {result[\"visited_points\"]:.1f}/{num_actions} points'\n",
    "        print(s_1 + s_2 + s_3)\n",
    "    print()\n",
    "    plt.plot(n_step_range, [result[\"temperature_function\"](n_step) for n_step in n_step_range], label=name)\n",
    "\n",
    "plt.hlines(0, n_step_range[0], n_step_range[-1])\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_trajectory(result, problems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = L.Trainer(\n",
    "#     precision=config[\"precision\"],\n",
    "#     enable_model_summary=False\n",
    "# )\n",
    "# trainer.test(model, dataloaders=val_online_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(checkpoint_file)\n",
    "# print(checkpoint['hyper_parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DPT Quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "\n",
    "from solvers.dpt.train import DPTSolver\n",
    "from dpt_run_2 import *\n",
    "# from dpt_run import *\n",
    "\n",
    "from problems import Quadratic, Net\n",
    "from utils import print_trajectory\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dpt_run_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from dpt_run_2 import *\n",
    "\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "train_offline_dataloader = get_offline_dataloader(config[\"train_histories_path\"], config)\n",
    "val_offline_dataloader = get_offline_dataloader(config[\"val_histories_path\"], config)\n",
    "print('train_offline_dataloader:', len(train_offline_dataloader.dataset))\n",
    "print('val_offline_dataloader:', len(val_offline_dataloader.dataset))\n",
    "\n",
    "# d = int(np.log2(config[\"model_params\"][\"num_actions\"]))\n",
    "# val_online_dataloader_1 = get_online_dataloader([Net(d=d, n=2, seed=i) for i in range(271, 281)], config)\n",
    "# val_online_dataloader_2 = get_online_dataloader([Net(d=d, n=2, seed=i) for i in range(1, 11)], config)\n",
    "# print('val_online_dataloader_1:', len(val_online_dataloader_1.dataset))\n",
    "# print('val_online_dataloader_2:', len(val_online_dataloader_2.dataset))\n",
    "\n",
    "# logger = WandbLogger(**config[\"wandb_params\"])\n",
    "# model = DPTSolver(config)\n",
    "\n",
    "# trainer = L.Trainer(\n",
    "#     logger=logger,\n",
    "#     precision=config[\"precision\"],\n",
    "#     max_epochs=config[\"max_epochs\"],\n",
    "#     log_every_n_steps=config[\"log_every_n_steps\"],\n",
    "#     default_root_dir=config[\"wandb_params\"][\"save_dir\"],\n",
    "#     enable_model_summary=False\n",
    "#     # deterministic=True\n",
    "# )\n",
    "# trainer.fit(\n",
    "#     model=model, \n",
    "#     train_dataloaders=train_offline_dataloader, \n",
    "#     val_dataloaders=[val_offline_dataloader,]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_offline_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['target_action'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
