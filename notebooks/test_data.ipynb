{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../'\n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "\n",
    "from scripts.create_problem import load_problem_set\n",
    "from scripts.run_solver import load_results\n",
    "from notebooks.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if a dataset (train, validation or test) contains separable problem classes. It means that if the dataset consists of $n$ different QUBO problem types, for example, Knapsack, MaxCut, etc, the corresponding martixes Q make spatially separable groups in the $\\mathbb{R}^{d \\times d}$ space according to their problem type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_dir = '../data/test'\n",
    "# read_dir = \"../data/normal_10\"\n",
    "# read_dir = \"../data/various\"\n",
    "read_dir = \"../data/normal\"\n",
    "# read_dir = \"../data/distributions\"\n",
    "problem_list = natsorted(os.listdir(read_dir))\n",
    "\n",
    "# problem_list = [\n",
    "#     'SPP',\n",
    "#     'GraphColoring',\n",
    "#     'Knapsack',\n",
    "#     'MVC',\n",
    "#     'QUBO',\n",
    "#     'SetPack',\n",
    "#     'WMaxCut',\n",
    "#     'NumberPartitioning',\n",
    "#     'MaxCut',\n",
    "#     'WMVC',\n",
    "#     'Ising',\n",
    "#     'Max2Sat',\n",
    "#     'QAP'\n",
    "# ]\n",
    "\n",
    "X, y = get_Xy(read_dir, problem_list, suffix='test')\n",
    "X_tsne = get_tsne(X)\n",
    "show_tsne(problem_list, X_tsne, y)\n",
    "print_unique(read_dir, problem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the optimization trajectoriy for every problem from the dataset obtained with different solvers and average them over a problem type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_dir = '../results/test'\n",
    "# read_dir = '../results/normal_10'\n",
    "# read_dir = '../results/various'\n",
    "read_dir = '../results/normal'\n",
    "# read_dir = '../results/distributions'\n",
    "\n",
    "problem_list = natsorted(os.listdir(read_dir))\n",
    "solver_list = ('RandomSearch', 'PSO', 'PROTES')\n",
    "\n",
    "meta_results = defaultdict(dict)\n",
    "meta_stats = defaultdict(dict)\n",
    "for problem in problem_list:\n",
    "    for solver in solver_list:\n",
    "        meta_results[problem][solver] = get_meta_results(problem, solver, read_dir, suffix='test', budget=5000, n_steps=100)\n",
    "        meta_stats[problem][solver] = get_trajectory_stats(problem, solver, read_dir, suffix='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here one can go to `utils.py` file and adjust the printing settings. Namely, it is possible to turn off clipping or add information about std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_meta_results(meta_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get one trajectory for a convinient printing average over the problem types as well. Also the trajectories statistics are accumulated as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Normal'\n",
    "problem_averaged_meta_results = get_problem_averaged_meta_dict(meta_results, title)\n",
    "problem_averaged_meta_stats = get_problem_averaged_meta_dict(meta_stats, title)\n",
    "\n",
    "show_meta_results(problem_averaged_meta_results)\n",
    "pd.DataFrame(problem_averaged_meta_stats[title]).T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened = {\n",
    "#     (row_key, col_key): subdict\n",
    "#     for row_key, cols in meta_stats.items()\n",
    "#     for col_key, subdict in cols.items()\n",
    "# }\n",
    "# pd.DataFrame.from_dict(flattened, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.create_problem import load_problem_set\n",
    "# from scripts.run_solver import load_results\n",
    "\n",
    "# d_x = []\n",
    "# problem_name = 'QAP'\n",
    "# problems = load_problem_set(f'{read_dir}/{problem_name}/train')\n",
    "# for problem in problems:\n",
    "#     d_x.append(problem.info['x_best'])\n",
    "#     print(problem.Q)\n",
    "# x_unique = torch.unique(torch.stack(d_x), dim=0)\n",
    "# print(x_unique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
