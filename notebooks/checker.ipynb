{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../'\n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.create_problem import load_problem_set\n",
    "import problems as pbs\n",
    "import solvers as sv\n",
    "from dpt.data import *\n",
    "from dpt.reward import Reward, ZeroReward\n",
    "from train_dpt import *\n",
    "from notebooks.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = 'Normal(0, 1)'\n",
    "data_dir = '../../data/normal'\n",
    "results_dir = '../../trajectories/normal'\n",
    "suffix = 'val'\n",
    "\n",
    "problems = load_problem_set(os.path.join(data_dir, problem, suffix))\n",
    "dataset = OfflineDataset(problems, seq_len=5, results_dir=results_dir, suffix=suffix, action='point', target_action='next')\n",
    "# dataset = OnlineDataset(problems)\n",
    "sample = dataset[0]\n",
    "\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.dtype, v.shape)\n",
    "    else:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = ZeroReward()\n",
    "rewards = reward_model.offline(sample['states'], sample['actions'], sample['next_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "print_sample(sample, rewards, 'point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all(sample['target_action'][-1] == sample['target_state'][:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = partial(custom_collate_fn, problem_class=getattr(pbs, 'Problem'))\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=10, collate_fn=collate_fn)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.dtype, v.shape)\n",
    "    else:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = reward_model.offline(\n",
    "    states=batch[\"states\"],\n",
    "    actions=batch[\"actions\"],\n",
    "    next_states=batch[\"next_states\"]\n",
    ")\n",
    "rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_dpt import DPTSolver, load_config\n",
    "\n",
    "config = load_config('../configs/dpt_50.yaml')\n",
    "# config['batch_size'] = 3\n",
    "config['action'] = \"point\"\n",
    "config['target_action'] = \"next\"\n",
    "config['model_params']['action_dim'] = 50\n",
    "\n",
    "# datamodule = ProblemDataModule(config)\n",
    "# datamodule.setup()\n",
    "\n",
    "# batch = next(iter(datamodule.test_dataloader()))\n",
    "\n",
    "model = DPTSolver(config)\n",
    "model.configure_model()\n",
    "\n",
    "outputs = model.model(\n",
    "    query_state=batch[\"query_state\"],\n",
    "    states=batch[\"states\"],\n",
    "    actions=batch[\"actions\"],\n",
    "    next_states=batch[\"next_states\"],\n",
    "    rewards=rewards,\n",
    ")\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# target_actions = [torch.argmax(sample['target_action']).item() for _ in range(100) for sample in dataset]\n",
    "# bins = np.arange(0, 12)\n",
    "# plt.hist(target_actions, bins-0.5)\n",
    "# plt.xticks(bins[:-1])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
