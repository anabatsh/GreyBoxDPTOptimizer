{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook to test a DPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../'\n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "from train_dpt import DPTSolver\n",
    "from notebooks.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a model from a checkpoint. The last checkpoint is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint(run_name):\n",
    "    read_dir = os.path.join(\"../../results\", \"DPT_3\", run_name, \"checkpoints\")\n",
    "    checkpoint = natsorted(os.listdir(read_dir))[-1]\n",
    "    checkpoint = os.path.join(read_dir, checkpoint)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "run_name = \"bitflip_gt_ce\"\n",
    "model_name = \"DPT\"\n",
    "checkpoint = get_checkpoint(run_name)\n",
    "model = DPTSolver.load_from_checkpoint(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 5 * model.config[\"model_params\"][\"seq_len\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model in an online inference mode for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = \"../data/normal\"\n",
    "problem_list = natsorted(os.listdir(read_dir))\n",
    "\n",
    "model_results = defaultdict(dict)\n",
    "for problem in problem_list:\n",
    "    model_results[problem] = run_model(model, read_dir, problem, name=model_name, budget=budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../final_results/normal'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_name = f'{model_name}_{run_name}_1k'\n",
    "save_path = os.path.join(save_dir, save_name + '.npy')\n",
    "np.save(save_path, model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the results according to the problems. In distribution problems are those we used to train the model on, and out of distribution problems are those the model did't see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_problems = model.config[\"exclude_problems\"]\n",
    "include_problems = [problem for problem in problem_list if problem not in exclude_problems]\n",
    "\n",
    "scaled_model_results = scale_meta_dict(model_results)\n",
    "model_id_results = {k: scaled_model_results[k] for k in include_problems}\n",
    "model_ood_results = {k: scaled_model_results[k] for k in exclude_problems}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average the results over the problems to get a common trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name = 'In distribution problems'\n",
    "ood_name = 'Out of distribution problems'\n",
    "model_results_averaged = {\n",
    "    id_name: get_problem_averaged_meta_dict(model_id_results),\n",
    "    ood_name: get_problem_averaged_meta_dict(model_ood_results)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the results. Here one can go to `utils.py` file and adjust the printing settings. Namely, it is possible to move the legend or turn on clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_meta_results(model_results_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the other solvers results and split them according to the problems. Then average them over the problems as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = \"../results/normal\"\n",
    "solver_list = ('PSO', 'PROTES', 'GUROBI')\n",
    "\n",
    "solver_results = defaultdict(dict)\n",
    "for problem in problem_list:\n",
    "    for solver in solver_list:\n",
    "        solver_results[problem][solver] = get_meta_results(problem, solver, read_dir, suffix='test', budget=budget, n_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = 'solvers'\n",
    "save_path = os.path.join(save_dir, save_name + '.npy')\n",
    "# if not os.path.exists(save_path):\n",
    "np.save(save_path, solver_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = 'DPT (warmup) (sample)'\n",
    "results = {\n",
    "    problem: {'DPT': model_results[problem][best_model]} | solver_results[problem] \n",
    "    for problem in problem_list\n",
    "}\n",
    "scaled_results = scale_meta_dict(results)\n",
    "\n",
    "id_results = {k: scaled_results[k] for k in include_problems}\n",
    "ood_results = {k: scaled_results[k] for k in exclude_problems}\n",
    "\n",
    "results_averaged = {\n",
    "    id_name: get_problem_averaged_meta_dict(id_results),\n",
    "    ood_name: get_problem_averaged_meta_dict(ood_results)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_meta_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_meta_results(results_averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup = 0\n",
    "# model.config[\"temperature\"] = lambda x: math.sqrt(x)\n",
    "# model.config[\"temperature\"] = lambda x: 5 - 4 * x\n",
    "# model.config[\"temperature\"] = lambda x: 1 / math.sqrt(1 + x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
