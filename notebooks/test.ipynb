{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root_path = '../'\n",
    "sys.path.insert(0, root_path)\n",
    "\n",
    "import lightning as L\n",
    "from natsort import natsorted\n",
    "\n",
    "from run import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"max_reward\"\n",
    "root_dir = os.path.join(\"../results\", \"DPT_3\", run_name, \"checkpoints\")\n",
    "checkpoint = natsorted(os.listdir(root_dir))[-1]\n",
    "checkpoint = os.path.join(root_dir, checkpoint)\n",
    "\n",
    "model = DPTSolver.load_from_checkpoint(checkpoint).cpu()\n",
    "config = model.config\n",
    "# config[\"problem_params\"][\"use_problems\"] = 1000 #250000\n",
    "datamodule = ProblemDataModule(config, path='..')\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config[\"online_steps\"] = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_inference(model, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    tester = L.Trainer(logger=False, precision=config[\"precision\"])\n",
    "    test_dataloader = datamodule.test_dataloader()\n",
    "\n",
    "    # check out two strategies of online inference:\n",
    "    # - where a predicted action is the argmax of a predicted distribution \n",
    "    # - where a predicted action is sampled with temperature = 1 from a predicted distribution \n",
    "    hparams = [\n",
    "        (\"argmax\", {\"do_sample\": False, \"temperature\": 0.0}),\n",
    "        (\"sampling\", {\"do_sample\": True, \"temperature\": 1.0}),\n",
    "    ]\n",
    "    for label, hparam in hparams:\n",
    "        model.config[\"do_sample\"] = hparam[\"do_sample\"]\n",
    "        model.config[\"temperature\"] = hparam[\"temperature\"]\n",
    "        tester.test(model=model, dataloaders=test_dataloader)\n",
    "        results = model.save_results\n",
    "        results = {\n",
    "            \"MAE(best x, x*)\": results[\"best_x_mae\"].cpu().tolist(),\n",
    "            \"MAE(best y, y*)\": results[\"best_y_mae\"].cpu().tolist(),\n",
    "            \"MAE(all x, x*)\": results[\"all_x_mae\"].cpu().tolist(),\n",
    "            \"MAE(all y, y*)\": results[\"all_y_mae\"].cpu().tolist(),\n",
    "        }\n",
    "        with open(f'{save_path}/{label}.json', 'w') as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'../results/DPT_3/{run_name}/online_inference'\n",
    "online_inference(model, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps as cm\n",
    "\n",
    "def plot(results, label, axes=None, c='red'):\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    axes[0].set_title('MAE (x, x*)')\n",
    "    axes[1].set_title('MAE (y, y*)')\n",
    "\n",
    "    axes[0].plot(results[\"MAE(all x, x*)\"], '-')\n",
    "    axes[0].plot(results[\"MAE(best x, x*)\"], '--')\n",
    "    axes[1].plot(results[\"MAE(all y, y*)\"], '-', label=f'{label} all')\n",
    "    axes[1].plot(results[\"MAE(best y, y*)\"], '--', label=f'{label} best')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlim(0, None)\n",
    "        ax.set_xlabel('Step')\n",
    "\n",
    "    axes[1].set_yscale('log', base=10)\n",
    "    axes[1].legend(loc=1)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = save_dir\n",
    "axes = None\n",
    "for file_name in os.listdir(read_dir):\n",
    "    file_path = os.path.join(read_dir, file_name)\n",
    "    with open(file_path) as f:\n",
    "        results = json.load(f)\n",
    "    label = file_name.split('.')[0]\n",
    "    axes = plot(results, label, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dpt reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = save_dir\n",
    "axes = None\n",
    "for file_name in os.listdir(read_dir):\n",
    "    file_path = os.path.join(read_dir, file_name)\n",
    "    with open(file_path) as f:\n",
    "        results = json.load(f)\n",
    "    label = file_name.split('.')[0]\n",
    "    axes = plot(results, label, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = save_dir\n",
    "axes = None\n",
    "for file_name in os.listdir(read_dir):\n",
    "    file_path = os.path.join(read_dir, file_name)\n",
    "    with open(file_path) as f:\n",
    "        results = json.load(f)\n",
    "    label = file_name.split('.')[0]\n",
    "    axes = plot(results, label, axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_offline_dataset = datamodule.train_dataloader().dataset\n",
    "val_offline_dataset = datamodule.val_dataloader().dataset\n",
    "val_online_dataset = datamodule.test_dataloader().dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, outputs, predictions, metrics = run(model, val_offline_dataset[0])\n",
    "print_sample(sample, predictions)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of online mode for a problem from the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one may choose a strategy\n",
    "model.config[\"do_sample\"] = False\n",
    "model.config[\"temperature\"] = 0.0\n",
    "# model.config[\"do_sample\"] = False\n",
    "# model.config[\"temperature\"] = 1.0\n",
    "\n",
    "sample, outputs, predictions, metrics = run(model, val_online_dataset[0], n_steps=50)\n",
    "print_sample(sample, print_ta=True, print_fm=True)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = val_offline_dataset[0][\"problem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = PROTES(problem=problem, budget=100, k_init=0, k_samples=50, k_top=5, seed=0)\n",
    "logs = solver.optimize()\n",
    "plt.plot(logs['m_list'], logs['y_list'], '-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = [\n",
    "    (\"PROTES\", partial(PROTES, budget=100, k_init=0, k_samples=50, k_top=5, seed=0)),\n",
    "    (\"OnePlusOne\", partial(OnePlusOne, budget=100, k_init=0, k_samples=1, seed=0)),\n",
    "    (\"PSO\", partial(PSO, budget=100, k_init=0, k_samples=1, seed=0)),\n",
    "    (\"NoisyBandit\", partial(NoisyBandit, budget=100, k_init=0, k_samples=1, seed=0)),\n",
    "    (\"SPSA\", partial(SPSA, budget=100, k_init=0, k_samples=1, seed=0)),\n",
    "    (\"Portfolio\", partial(Portfolio, budget=100, k_init=0, k_samples=1, seed=0)),\n",
    "]\n",
    "for name, solver in solvers:\n",
    "    logs = solver(problem=problem).optimize()\n",
    "    plt.plot(logs['m_list'], logs['y_list'], label=name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
