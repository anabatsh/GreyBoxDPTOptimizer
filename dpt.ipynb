{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash dpt_run.sh\n",
    "# the same thing !python3 solvers/dpt/train_dpt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from solvers.dpt.src.model_dpt import DPT_K2D\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "model = DPT_K2D(\n",
    "    num_states=1,\n",
    "    num_actions=16,\n",
    "    hidden_dim=512,\n",
    "    seq_len=50,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.5,\n",
    "    residual_dropout=0.1,\n",
    "    embedding_dropout=0.3,\n",
    "    normalize_qk=False,\n",
    "    pre_norm=True,\n",
    "    rnn_weights_path=None,\n",
    "    state_rnn_embedding=1,\n",
    "    rnn_dropout=0.0,\n",
    ").to(DEVICE)\n",
    "\n",
    "checkpoint_path = \"solvers/dpt/checkpoints/model_last.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from utils import int2bin\n",
    "from problems import Net\n",
    "from utils import get_xaxis\n",
    "\n",
    "\n",
    "p = Net(d=4, n=2, seed=100)\n",
    "all_targets = p.target(get_xaxis(d=4, n=2))\n",
    "\n",
    "query_states = torch.tensor([all_targets.max()])\n",
    "context_states = torch.Tensor(1, 0)\n",
    "context_actions = torch.Tensor(1, 0)\n",
    "context_rewards = torch.Tensor(1, 0)\n",
    "\n",
    "for _ in range(10):\n",
    "    predicted_actions = model(\n",
    "        query_states=query_states.to(dtype=torch.float, device=DEVICE),\n",
    "        context_states=context_states.to(dtype=torch.float, device=DEVICE),\n",
    "        context_next_states=None,\n",
    "        context_actions=context_actions.to(dtype=torch.long, device=DEVICE),\n",
    "        context_rewards=context_rewards.to(dtype=torch.float, device=DEVICE),\n",
    "    )    \n",
    "    predicted_action = torch.argmax(F.softmax(predicted_actions, dim=1)).cpu()\n",
    "    point = int2bin(predicted_action, d=4, n=2)\n",
    "    target = torch.tensor(p.target(point))\n",
    "    print(f'step {_} | current target: {query_states.item():>8.6} -> suggested point: {point} -> new target: {target.item():.6}')\n",
    "\n",
    "    context_states = torch.cat([context_states, target.unsqueeze(0)], dim=1)\n",
    "    context_actions = torch.cat([context_actions, torch.tensor([predicted_action]).unsqueeze(0)], dim=1)\n",
    "    context_rewards = torch.cat([context_rewards, (target - query_states).unsqueeze(0)], dim=1)\n",
    "    query_states = target\n",
    "\n",
    "print()\n",
    "print(f'found minimal value: {target.item():.6}')\n",
    "print(f'ground truth: {all_targets.min().item():.6}')\n",
    "print()\n",
    "print(f'all possible targets in an order:\\n{np.sort(all_targets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = MarkovianDataset(\"trajectories/\", seq_len=200)\n",
    "# # dataloader = DataLoader(dataset=dataset, batch_size=1, pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     (\n",
    "#         query_flag,\n",
    "#         query_states,\n",
    "#         flags,\n",
    "#         states,\n",
    "#         actions,\n",
    "#         next_flags,\n",
    "#         next_states,\n",
    "#         rewards,\n",
    "#         target_actions,\n",
    "#     ) = [b.to(dtype=torch.float, device=DEVICE) for b in batch]\n",
    "#     break\n",
    "\n",
    "# actions = actions.to(torch.long)\n",
    "\n",
    "# predicted_actions = model(\n",
    "#     query_states=query_states,\n",
    "#     context_states=states,\n",
    "#     context_next_states=next_states,\n",
    "#     context_actions=actions,\n",
    "#     context_rewards=rewards,\n",
    "# )\n",
    "\n",
    "# predicted_action = torch.argmax(F.softmax(predicted_actions, dim=1))\n",
    "# target_actions, predicted_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from solvers.dpt.src.utils.data import MarkovianDataset\n",
    "from solvers.dpt.src.model_dpt import DPT_K2D\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "dataset = MarkovianDataset('trajectories', seq_len=200)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=128,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPT_K2D(\n",
    "    num_states=1,\n",
    "    num_actions=10,\n",
    "    hidden_dim=512,\n",
    "    seq_len=200,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.5,\n",
    "    residual_dropout=0.1,\n",
    "    embedding_dropout=0.3,\n",
    "    normalize_qk=False,\n",
    "    pre_norm=True,\n",
    "    rnn_weights_path=None,\n",
    "    state_rnn_embedding=16,\n",
    "    rnn_dropout=0.0,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    (\n",
    "        query_flag,\n",
    "        query_states,\n",
    "        flags,\n",
    "        states,\n",
    "        actions,\n",
    "        next_flags,\n",
    "        next_states,\n",
    "        rewards,\n",
    "        target_actions,\n",
    "    ) = [b.to(DEVICE) for b in batch]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = query_states.to(torch.float)\n",
    "states = states.to(torch.float)\n",
    "actions = actions.to(torch.long)\n",
    "next_states = next_states.to(torch.float)\n",
    "rewards = rewards.to(torch.float32)\n",
    "\n",
    "target_actions = target_actions.squeeze(-1)\n",
    "target_actions = (\n",
    "    F.one_hot(target_actions, num_classes=10)\n",
    "    .unsqueeze(1)\n",
    "    .repeat(1, 200, 1)\n",
    "    .float()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_actions = model(\n",
    "    query_states=query_states,\n",
    "    context_states=states,\n",
    "    context_next_states=next_states,\n",
    "    context_actions=actions,\n",
    "    context_rewards=rewards,\n",
    ")\n",
    "predicted_actions = predicted_actions[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(\n",
    "    input=predicted_actions.flatten(0, 1),\n",
    "    target=target_actions.flatten(0, 1),\n",
    "    label_smoothing=0.0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
