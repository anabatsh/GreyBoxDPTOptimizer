{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no `results` directory, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash dpt_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/GreyBoxDPTOptimizer_/solvers/dpt/train.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from solvers.dpt.train import DPTSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPTSolver('config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training histories...\n",
      "Num histories: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manabatsheva\u001b[0m (\u001b[33manabatsheva_sk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Desktop/GreyBoxDPTOptimizer_/wandb/run-20241110_001442-2kp3pedi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anabatsheva_sk/GreyBoxDPT/runs/2kp3pedi' target=\"_blank\">worldly-puddle-80</a></strong> to <a href='https://wandb.ai/anabatsheva_sk/GreyBoxDPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anabatsheva_sk/GreyBoxDPT' target=\"_blank\">https://wandb.ai/anabatsheva_sk/GreyBoxDPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anabatsheva_sk/GreyBoxDPT/runs/2kp3pedi' target=\"_blank\">https://wandb.ai/anabatsheva_sk/GreyBoxDPT/runs/2kp3pedi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c706594913442c9bd860716a82097b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from problems import Net\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def transition_function(state, action, problem):\n",
    "    point = int2bin(action, d=problem.d, n=problem.n)\n",
    "    state = torch.tensor(problem.target(point))\n",
    "    return state\n",
    "\n",
    "def _eval_function(model, problem):\n",
    "    all_actions = get_xaxis(d=problem.d, n=problem.n)\n",
    "    all_states = problem.target(all_actions)\n",
    "    target_state = torch.tensor([all_states.min()])\n",
    "    query_state = torch.tensor([all_states.max()])\n",
    "\n",
    "    trajectory = model.test(query_state, partial(transition_function, problem=problem))\n",
    "\n",
    "    best_found_state = trajectory[3].min()\n",
    "    accuracy = (query_state - best_found_state) / (query_state - target_state)\n",
    "    return accuracy.item()\n",
    "\n",
    "def eval_function(model, problems, keys):\n",
    "    return {\n",
    "        key: _eval_function(model, problem) \n",
    "        for problem, key in zip(problems, keys)\n",
    "    }\n",
    "\n",
    "train_problem = Net(d=4, n=2, seed=1)\n",
    "test_problem = Net(d=4, n=2, seed=0)\n",
    "\n",
    "eval_function = partial(\n",
    "    eval_function, \n",
    "    problems=(train_problem, test_problem), \n",
    "    keys=('accuracy (train problem)', 'accuracy (test problem)')\n",
    ")\n",
    "model.train(eval_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 train_dpt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a model and run it on a `Net` problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GreyBoxDPTOptimizer_.test_dpt import load_model, test\n",
    "from problems import Net\n",
    "\n",
    "\n",
    "model = load_model(\"../GreyBoxDPTOptimizerData/checkpoints/model_last.pt\")\n",
    "model.eval()\n",
    "problem = Net(d=4, n=2, seed=1)\n",
    "result = test(model, problem, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it on a proble, of the class `Net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from utils import int2bin\n",
    "from problems import Net\n",
    "from utils import get_xaxis\n",
    "\n",
    "\n",
    "p = Net(d=4, n=2, seed=1)\n",
    "all_targets = p.target(get_xaxis(d=4, n=2))\n",
    "\n",
    "temp = 0.5\n",
    "do_samples = True\n",
    "\n",
    "query_states = torch.tensor([all_targets.max()])\n",
    "context_states = torch.Tensor(1, 0)\n",
    "context_next_states = torch.Tensor(1, 0)\n",
    "context_actions = torch.Tensor(1, 0)\n",
    "context_rewards = torch.Tensor(1, 0)\n",
    "\n",
    "for _ in range(10):\n",
    "    predicted_actions = model(\n",
    "        query_states=query_states.to(dtype=torch.float, device=DEVICE),\n",
    "        context_states=context_states.to(dtype=torch.float, device=DEVICE),\n",
    "        context_next_states=context_next_states.to(dtype=torch.float, device=DEVICE),\n",
    "        context_actions=context_actions.to(dtype=torch.long, device=DEVICE),\n",
    "        context_rewards=context_rewards.to(dtype=torch.float, device=DEVICE),\n",
    "    )\n",
    "    temp = 1.0 if temp <= 0 else temp\n",
    "    probs = F.softmax(predicted_actions / temp, dim=-1)\n",
    "    if do_samples:\n",
    "        predicted_action = torch.multinomial(probs, num_samples=1).squeeze(1).cpu()[0]\n",
    "    else:\n",
    "        predicted_action = torch.argmax(probs, dim=-1).cpu()[0]\n",
    "\n",
    "    # predicted_action = torch.argmax(F.softmax(predicted_actions, dim=1)).cpu()\n",
    "    # делать сэмплирование из распределения, а не брать моду\n",
    "    # добавить температуру\n",
    "    point = int2bin(predicted_action, d=4, n=2)\n",
    "    target = torch.tensor(p.target(point))\n",
    "    print(f'step {_} | current target: {query_states.item():>8.6} -> suggested point: {point} -> new target: {target.item():.6}')\n",
    "\n",
    "    context_states = torch.cat([context_states, query_states.unsqueeze(0)], dim=1)\n",
    "    context_next_states = torch.cat([context_next_states, target.unsqueeze(0)], dim=1)\n",
    "    context_actions = torch.cat([context_actions, torch.tensor([predicted_action]).unsqueeze(0)], dim=1)\n",
    "    context_rewards = torch.cat([context_rewards, (target - query_states).unsqueeze(0)], dim=1)\n",
    "    query_states = target\n",
    "\n",
    "print()\n",
    "print(f'found minimal value: {target.item():.6}')\n",
    "print(f'ground truth: {all_targets.min().item():.6}')\n",
    "print()\n",
    "print(f'all possible targets in an order:\\n{np.sort(all_targets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other extra things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset = MarkovianDataset(\"trajectories/\", seq_len=200)\n",
    "# # dataloader = DataLoader(dataset=dataset, batch_size=1, pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     (\n",
    "#         query_flag,\n",
    "#         query_states,\n",
    "#         flags,\n",
    "#         states,\n",
    "#         actions,\n",
    "#         next_flags,\n",
    "#         next_states,\n",
    "#         rewards,\n",
    "#         target_actions,\n",
    "#     ) = [b.to(dtype=torch.float, device=DEVICE) for b in batch]\n",
    "#     break\n",
    "\n",
    "# actions = actions.to(torch.long)\n",
    "\n",
    "# predicted_actions = model(\n",
    "#     query_states=query_states,\n",
    "#     context_states=states,\n",
    "#     context_next_states=next_states,\n",
    "#     context_actions=actions,\n",
    "#     context_rewards=rewards,\n",
    "# )\n",
    "\n",
    "# predicted_action = torch.argmax(F.softmax(predicted_actions, dim=1))\n",
    "# target_actions, predicted_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(f\"Problem {i}...\")\n",
    "    !bash ./run.sh $i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers.dpt.src.utils.data import results2trajectories\n",
    "\n",
    "results2trajectories('results', 'solvers/dpt/trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from solvers.dpt.src.utils.data import MarkovianDataset\n",
    "from GreyBoxDPTOptimizer_.solvers.dpt.model import DPT_K2D\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "dataset = MarkovianDataset('solvers/dpt/trajectories', seq_len=50)\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=4,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPT_K2D(\n",
    "    num_states=1,\n",
    "    num_actions=16,\n",
    "    hidden_dim=512,\n",
    "    seq_len=50,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.5,\n",
    "    residual_dropout=0.1,\n",
    "    embedding_dropout=0.3,\n",
    "    normalize_qk=False,\n",
    "    pre_norm=True,\n",
    "    rnn_weights_path=None,\n",
    "    state_rnn_embedding=16,\n",
    "    rnn_dropout=0.0,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    (\n",
    "        query_states,\n",
    "        states,\n",
    "        actions,\n",
    "        next_states,\n",
    "        rewards,\n",
    "        target_actions,\n",
    "    ) = [b.to(DEVICE) for b in batch]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = query_states.to(torch.float)\n",
    "states = states.to(torch.float)\n",
    "actions = actions.to(torch.long)\n",
    "next_states = next_states.to(torch.float)\n",
    "rewards = rewards.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_actions_onehot = (\n",
    "    F.one_hot(target_actions.squeeze(-1), num_classes=16)\n",
    "    .unsqueeze(1)\n",
    "    .repeat(1, 50, 1)\n",
    "    .float()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_actions = model(\n",
    "    query_states=query_states,\n",
    "    context_states=states,\n",
    "    context_next_states=next_states,\n",
    "    context_actions=actions,\n",
    "    context_rewards=rewards,\n",
    ")\n",
    "predicted_actions = predicted_actions[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(\n",
    "    input=predicted_actions.flatten(0, 1),\n",
    "    target=target_actions_onehot.flatten(0, 1),\n",
    "    label_smoothing=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "logits = torch.tensor(np.hstack([np.arange(5), np.arange(4)[::-1]])).to(torch.float)\n",
    "d = len(logits)\n",
    "bins = np.arange(d+1)\n",
    "N = 1000\n",
    "\n",
    "temp = 1\n",
    "probs = torch.nn.functional.softmax(logits / temp, dim=-1)\n",
    "next_tokens = torch.multinomial(probs, num_samples=N, replacement=True)#.squeeze(1)\n",
    "distr = np.histogram(next_tokens, bins=bins)[0] / N\n",
    "plt.plot(bins[:-1]+0.5, distr, '-o', label=f't={temp}')\n",
    "# plt.hist(next_tokens, bins=bins, density=True, rwidth=0.9)\n",
    "\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "next_tokens = torch.argmax(logits, dim=-1)\n",
    "distr = np.zeros(d)\n",
    "distr[next_tokens] = 1\n",
    "plt.plot(bins[:-1]+0.5, distr, '-o', label='argmax')\n",
    "# plt.hist([next_tokens], bins=bins, density=False, rwidth=0.9)\n",
    "\n",
    "# plt.xticks(bins[:-1]+0.5, bins[:-1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
